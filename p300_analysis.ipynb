{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4f07914",
   "metadata": {},
   "source": [
    "# P300 analysis from OpenVibe/BCI2000\n",
    "With lots of cool preprocessing features.\n",
    "source: [https://github.com/lokinou/p300_analysis_from_openvibe](https://github.com/lokinou/p300_analysis_from_openvibe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84d777ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the line wit qt below to obtain separate plots to save\n",
    "%matplotlib inline\n",
    "%matplotlib qt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46d2b757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if packages are missing, uncomment and execute here or in anaconda prompt with p300mne env\n",
    "#!pip install \"git+https://github.com/nbara/python-meegkit\"\n",
    "#!pip install statsmodels pyriemann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64932226",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import mne\n",
    "import itertools\n",
    "import re\n",
    "from pathlib import Path\n",
    "# LDA\n",
    "from sklearn import metrics\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866800c5",
   "metadata": {},
   "source": [
    "## Define the analysis variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffefc7d",
   "metadata": {},
   "source": [
    "if you don't know how to convert the .ov files, please check my [ov to gdf tutorial](https://github.com/lokinou/openvibe_to_gdf_tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abb2827a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the gdf files\n",
    "#data_dir=r\"C:\\BCI\\dev\\p300_analysis_from_openvibe\"\n",
    "data_dir=r\"./data_sample\"\n",
    "data_type= None  # bci2000 or openvibe or None for autodetection\n",
    "\n",
    "# Define the electrodes here (for the provided sample file)\n",
    "cname = None\n",
    "cname = ['Fz', 'FC1', 'FC2', 'C1', 'Cz', 'C2', 'P3', 'Pz', 'P4', 'Oz']\n",
    "#cname = ['Fz', 'Cz', 'P3', 'Pz', 'P4', 'PO7', 'PO8', 'Oz']\n",
    "\n",
    "# Visual\n",
    "skip_slow_ERP_plot = False  # skip the channelwise ERP plot\n",
    "display_preprocessing_plots = True\n",
    "display_all_erp_plots = True\n",
    "export_figures = True\n",
    "fig_folder = './out'\n",
    "\n",
    "# Preprocessing\n",
    "apply_resample = True # in case the sampling rate is high (>=256Hz)\n",
    "resample_freq = 256 # Hz\n",
    "apply_infinite_reference = False  # rereferencing\n",
    "apply_ASR = True  # use Artifact Subspace Reconstruction (artifact removal)\n",
    "apply_CSD = False  # use Current Source Density (spatial filter)\n",
    "\n",
    "drop_bad_epochs = False\n",
    "reject_channels_full_of_artifacts = True\n",
    "reject_artifactual_epochs = reject_channels_full_of_artifacts and True # do not reject epochs if you dont reject channels or use CSD\n",
    "artifact_threshold = 100e-6\n",
    "ratio_tolerated_artifacts = 0.3  # if 30% of artifacts in 200ms windows, then the channel is rejected\n",
    "\n",
    "# ERP analysis parameters (values in sec)\n",
    "pre_epoch = -.2\n",
    "epoch_length = .6\n",
    "baseline = (-.2, 0)\n",
    "#isi = .0625\n",
    "#flash = .125\n",
    "\n",
    "\n",
    "# LDA\n",
    "resample_LDA = 32 # Hz\n",
    "nb_k_splits = 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e2ad521-f8ab-4575-8048-e2f6ce9263eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For internal processing, stimuli begin at 100 to discriminate from MNE usually using stimuli 1 and 0 as target and non-target \n",
    "stimulus_padding  = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0041485e",
   "metadata": {},
   "source": [
    "## Load the data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6eda1f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BCI2kReader import BCI2kReader as b2k\n",
    "from BCI2kReader import FileReader as f2k\n",
    "\n",
    "raws = []\n",
    "\n",
    "def extract_annotations(filename, verbose=False):\n",
    "    display_preprocessing_plots = False\n",
    "    file= b2k.BCI2kReader(filename)\n",
    "\n",
    "    if verbose:\n",
    "        print(file.states)\n",
    "    target_states = np.squeeze(file.states['StimulusType'])\n",
    "    stimulus_codes = np.squeeze(file.states['StimulusCode'])\n",
    "    if 'StimulusBegin' in file.states.keys():\n",
    "        stimulus_begin = np.squeeze(file.states['StimulusBegin'])\n",
    "    else:\n",
    "        stimulus_begin = np.squeeze(file.states['Flashing'])\n",
    "\n",
    "    phase = np.squeeze(file.states['PhaseInSequence'])\n",
    "\n",
    "    fs = file.samplingrate\n",
    "\n",
    "    idx_targets = np.where(target_states)[0]\n",
    "    idx_codes = np.where(stimulus_codes>0)[0]\n",
    "    idx_begin = np.where(stimulus_begin>0)[0]\n",
    "\n",
    "\n",
    "    # In BCI2000 states are maintained over different samples, we search here the differences of when the codes are > 0\n",
    "    groups = np.split(idx_codes, np.where(np.diff(idx_codes) != 1)[0]+1)\n",
    "    # we take the first sample where a difference can be found\n",
    "    code_change_idx = np.array([g[0] for g in groups])\n",
    "    #[idx_codes[idx] for idx in code_change_idx]\n",
    "    print('nb stimuli={}'.format(len(code_change_idx)))\n",
    "\n",
    "    # we intersect the target index list with the code change to find the onset of targets and non-targets\n",
    "    target_idx=np.intersect1d(code_change_idx,idx_targets)\n",
    "    print('nb targets={}'.format(len(target_idx)))\n",
    "    non_target_idx= np.setdiff1d(code_change_idx,idx_targets)\n",
    "\n",
    "    # Translating into MNE Annotations \n",
    "    # define the annotations from the recovered stimuli (in seconds)\n",
    "    sample_lengh = 1/fs\n",
    "    onsets = code_change_idx * sample_lengh\n",
    "    onsets = np.repeat(onsets, 2)  # repeat onsets\n",
    "    # define the descriptio\n",
    "    description_targets = np.zeros(code_change_idx.shape, dtype=np.uint)\n",
    "    # index of targets in the list of stimuli onsets\n",
    "    description_targets[np.searchsorted(code_change_idx, target_idx)] = 1\n",
    "    description_codes = stimulus_codes[code_change_idx] + stimulus_padding  # start codes at 100 because 0 and 1 are used for target and nontarget\n",
    "    # merge code and target decriptions\n",
    "    description = np.zeros(description_targets.shape[0]*2, dtype=np.uint)\n",
    "    description[np.arange(description_targets.shape[0]*2, step=2)] = description_codes\n",
    "    description[np.arange(start=1, stop=(description_targets.shape[0]*2)+1, step=2)] = description_targets\n",
    "\n",
    "    if display_preprocessing_plots:\n",
    "        fig = plt.figure()\n",
    "        plt.plot(description[:100])\n",
    "        fig.suptitle('Targets(1) and non-targets(0) for 100 first stimuli')\n",
    "\n",
    "    if display_preprocessing_plots:\n",
    "        fig = plt.figure()\n",
    "        plt.plot(phase == 1)\n",
    "        fig.suptitle('Trial begin')\n",
    "\n",
    "    # extract trial begin markers  #  this method does not work since some stimuli are declared before phase==1\n",
    "    # let's think baclwards use the end markers instead\n",
    "    new_phase_continuous = np.where(phase == 1)[0]\n",
    "    groups = np.split(new_phase_continuous, np.where(np.diff(new_phase_continuous) != 1)[0]+1)\n",
    "    new_trial_idx = np.array([g[0] for g in groups])\n",
    "    \n",
    "    \n",
    "    # extract trial end markers\n",
    "    new_phase_continuous = np.where(phase == 3)[0]\n",
    "    groups = np.split(new_phase_continuous, np.where(np.diff(new_phase_continuous) != 1)[0]+1)\n",
    "    end_of_trial_idx = np.array([g[-1] for g in groups]) # take the last index to integrate all post sequence duration\n",
    "    \n",
    "    # deduce trial begin markers  # \n",
    "    #new_trial_idx = np.zeros(end_of_trial_idx.size)\n",
    "    #new_trial_idx[1:] = end_of_trial_idx[1:]+1\n",
    "\n",
    "    if new_trial_idx.shape[0] > end_of_trial_idx[0]:\n",
    "        print('WARNING: no end of trial for the last trial (interrupted recording?), it will be ignored for offline accuracy calculation')\n",
    "        inter_trial_duration = end_of_trial_idx[0:len(new_trial_idx)] - new_trial_idx\n",
    "    else:\n",
    "        inter_trial_duration = end_of_trial_idx - new_trial_idx\n",
    "    inter_trial_duration = inter_trial_duration * sample_lengh  # express in seconds\n",
    "\n",
    "\n",
    "    print(\"Extracted {} trials\".format(len(new_trial_idx)))\n",
    "\n",
    "    # set a non-zero duration for stimuli (or MNE ignores them)\n",
    "    duration = np.ones(onsets.shape) * sample_lengh\n",
    "\n",
    "\n",
    "    # merge phase in sequence events with stimuli onsets\n",
    "    onsets_phase = new_trial_idx * sample_lengh\n",
    "    onsets = np.concatenate((onsets, onsets_phase))\n",
    "    \n",
    "    duration = np.concatenate((duration, inter_trial_duration))\n",
    "    description = np.concatenate((description, np.ones(new_trial_idx.shape) * 10))  # concatenate trials markers=10\n",
    "    srt = np.argsort(onsets) # sort according to their timing\n",
    "    onsets=onsets[srt]\n",
    "    duration = duration[srt]\n",
    "    description = description[srt].astype(np.uint8)\n",
    "    inter_trial_duration\n",
    "    annotations = mne.Annotations(onset=onsets, duration=duration, description=description)\n",
    "\n",
    "    file.flush()\n",
    "    return annotations\n",
    "\n",
    "def load_bci2k(filename_list, verbose=False):\n",
    "    \"\"\"\n",
    "    return MNE raw, number of rows in the matrix\n",
    "    \"\"\"\n",
    "    raws = []\n",
    "    for fn in filename_list:\n",
    "        cname = None\n",
    "        with b2k.BCI2kReader(fn) as file:\n",
    "            \n",
    "            # Extract signals and states\n",
    "            print('opened')\n",
    "            eeg_data = file.signals\n",
    "            states = file.states\n",
    "            fs = file.samplingrate\n",
    "            nb_chan = eeg_data.shape[0]\n",
    "            #file.purge()\n",
    "\n",
    "            # Extract channel names\n",
    "            reader = f2k.bcistream(fn)\n",
    "            if verbose:\n",
    "                print(reader.params)\n",
    "            # actualize the parameters by including the defined channel names\n",
    "            if len(reader.params['ChannelNames']):\n",
    "                if cname != reader.params['ChannelNames']:\n",
    "                    cname = reader.params['ChannelNames']\n",
    "                    print('Actualized channel names to {}'.format(cname))\n",
    "\n",
    "            if cname is None:\n",
    "                cname = [str(ch_n) for ch_n in list(range(nb_chan))]\n",
    "                \n",
    "            # extract the number of rows\n",
    "            nb_stim_rows = np.uint8(reader.params['NumMatrixRows'][0])\n",
    "            nb_stim_cols = np.uint8(reader.params['NumMatrixColumns'][0])\n",
    "            nb_seq = np.uint8(reader.params['NumberOfSequences'])\n",
    "\n",
    "            # convert states into annotations\n",
    "            info = mne.create_info(cname, fs, ch_types='eeg', verbose=None)\n",
    "            raw_array = mne.io.RawArray(eeg_data, info)\n",
    "            # Manually force the filename or mne complains\n",
    "            raw_array._filenames = [os.path.basename(fn)]\n",
    "            \n",
    "            annotations = extract_annotations(fn, verbose= False)\n",
    "            raw_array.set_annotations(annotations)\n",
    "            raws.append(raw_array)\n",
    "    return raws, (nb_stim_rows, nb_stim_cols, nb_seq)\n",
    "\n",
    "#fn = [\"./data_sample/bci2000\\Heide_einsteinBP_calibration4S001R01.dat\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a34fae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: failed to interpret \"auto\" as type int in parameter \"VisualizeSourceBufferSize\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data_sample\\bp2calib.dat\n",
      "['./data_sample\\\\bp2calib.dat']\n",
      "opened\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: failed to interpret \"auto\" as type int in parameter \"VisualizeSourceBufferSize\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SourceCh': 10, 'SampleBlockSize': 16, 'SamplingRate': 256, 'ChannelNames': [], 'SourceChOffset': (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0), 'SourceChGain': (1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0), 'DataDirectory': 'C:\\\\BCI\\\\bp2\\\\src\\\\users\\\\rubenmols\\\\data', 'SubjectName': 'rubenmols', 'SubjectSession': '001', 'SubjectRun': '01', 'ID_System': '', 'ID_Amp': '', 'ID_Montage': '', 'VisualizeTiming': 1, 'VisualizeSource': 1, 'VisualizeSourceDecimation': 1, 'VisualizeSourceBufferSize': None, 'VisualizeSourceTime': 2000, 'SourceMin': -100, 'SourceMax': 100, 'SourceChList': [], 'SourceChDevices': (10,), 'NumBuffers': 5, 'DeviceIDMaster': 'auto', 'FilterEnabled': 1, 'FilterHighPass': 0.1, 'FilterLowPass': 60.0, 'FilterModelOrder': 8, 'FilterType': 1, 'NotchEnabled': 1, 'NotchHighPass': 48.0, 'NotchLowPass': 52.0, 'NotchModelOrder': 4, 'NotchType': 1, 'DeviceIDs': ['auto'], 'DigitalInput': 0, 'DigitalOutput': 0, 'DigitalOutputEx': '', 'SignalType': 1, 'AcquisitionMode': 0, 'CommonGround': 1, 'CommonReference': 1, 'FileFormat': 'dat', 'StorageTime': '2020-10-06T13:26:25', 'AlignChannels': 1, 'SourceChTimeOffset': [], 'TransmitChList': ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10'], 'EventLink': 1, 'DataFile': 'rubenmolsB1\\\\2020-10-06\\\\bp2(2)calib.dat', 'OperatorIP': '127.0.0.1', 'SignalSourcePort': '50471', 'SignalSourceIP': '127.0.0.1', 'SignalSourceVersion': [['3.05'], [', '], ['MSVC-15.0.30729.1-AMD64, release, Aug 20 2015 15:13:41, lokinou@DESKTOP-LNFB303'], ['USE_DYNAMIC_IMPORTS:ON USE_SSE2:OFF USE_PRECOMPILED_HEADERS:ON']], 'SignalSourceFilterChain': [['DataIOFilter', '0'], ['gUSBampADC', '1'], ['NullFileWriter', '1'], ['BCI2000FileWriter', '1'], ['EDFFileWriter', '1'], ['GDFFileWriter', '1'], ['AlignmentFilter', '1.1'], ['TransmissionFilter', '1.2']], 'Filters': [['/1/DataIOFilter'], ['/1/gUSBampADC'], ['/1/BCI2000FileWriter'], ['/1/AlignmentFilter'], ['/1/TransmissionFilter'], ['/3/ConnectorInput'], ['/3/P3SpellerTask'], ['/3/KeystrokeFilter'], ['/3/ConnectorOutput'], ['/2/SpatialFilter'], ['/2/P3TemporalFilter'], ['/2/LinearClassifier']], 'VisualizeAlignmentFilter': 0, 'VisualizeTransmissionFilter': 0, 'VisPropertySets': [], 'OperatorVersion': [['3.05'], [', '], ['MSVC-15.0.30729.1-AMD64, release, Aug 20 2015 15:13:41, lokinou@DESKTOP-LNFB303'], ['USE_DYNAMIC_IMPORTS:ON USE_SSE2:OFF USE_PRECOMPILED_HEADERS:ON'], ['000007FED5D2EC70']], 'OperatorBackLink': 1, 'AutoConfig': 1, 'ConnectorInputFilter': [], 'ConnectorInputAddress': '', 'WindowWidth': 0, 'WindowHeight': 0, 'WindowLeft': 0, 'WindowTop': 0, 'WindowBackgroundColor': '0x000000', 'VisualizeApplicationWindow': 0, 'AppWindowSpatialDecimation': 8, 'AppWindowTemporalDecimation': 4, 'ShowAppLog': 1, 'PreRunDuration': 2000.0, 'PostRunDuration': 2000.0, 'PreSequenceDuration': 2000.0, 'PostSequenceDuration': 3000.0, 'StimulusDuration': 62.5, 'EarlyOffsetExpression': '', 'ISIMinDuration': 125.0, 'ISIMaxDuration': 125.0, 'InterpretMode': 2, 'DisplayResults': 0, 'AccumulateEvidence': 0, 'MinimumEvidence': 0.0, 'NumberOfSequences': 15, 'TargetDefinitions': [['A', 'A', '1', '', ''], ['B', 'B', '1', '', ''], ['C', 'C', '1', '', ''], ['D', 'D', '1', '', ''], ['E', 'E', '1', '', ''], ['F', 'F', '1', '', ''], ['G', 'G', '1', '', ''], ['H', 'H', '1', '', ''], ['I', 'I', '1', '', ''], ['J', 'J', '1', '', ''], ['K', 'K', '1', '', ''], ['L', 'L', '1', '', ''], ['M', 'M', '1', '', ''], ['N', 'N', '1', '', ''], ['O', 'O', '1', '', ''], ['P', 'P', '1', '', ''], ['Q', 'Q', '1', '', ''], ['R', 'R', '1', '', ''], ['S', 'S', '1', '', ''], ['T', 'T', '1', '', ''], ['U', 'U', '1', '', ''], ['V', 'V', '1', '', ''], ['W', 'W', '1', '', ''], ['X', 'X', '1', '', ''], ['Y', 'Y', '1', '', ''], ['Z', 'Z', '1', '', ''], ['0', '0', '1', '', ''], ['1', '1', '1', '', ''], ['2', '2', '1', '', ''], ['3', '3', '1', '', ''], ['4', '4', '1', '', ''], ['5', '5', '1', '', ''], ['6', '6', '1', '', ''], ['7', '7', '1', '', ''], ['8', '8', '1', '', ''], ['9', '9', '1', '', ''], [';', ';', '1', '', ''], ['.', '.', '1', '', ''], ['>', '>', '1', '', ''], ['_', '_', '1', '', ''], ['!', '!', '1', '', ''], ['&', '&', '1', '', ''], ['$', '$', '1', '', ''], ['*', '*', '1', '', ''], ['?', '?', '1', '', ''], ['%', '%', '1', '', ''], ['(', '(', '1', '', ''], [')', ')', '1', '', '']], 'NumMatrixColumns': (8,), 'NumMatrixRows': (6,), 'AudioStimuliOn': 0, 'AudioStimuliRowsFiles': [['sounds\\\\1.wav'], ['sounds\\\\2.wav'], ['sounds\\\\3.wav'], ['sounds\\\\4.wav'], ['sounds\\\\5.wav'], ['sounds\\\\6.wav']], 'AudioStimuliColsFiles': [['sounds\\\\a.wav'], ['sounds\\\\b.wav'], ['sounds\\\\c.wav'], ['sounds\\\\d.wav'], ['sounds\\\\e.wav'], ['sounds\\\\f.wav']], 'TargetWidth': (12.0,), 'TargetHeight': (14.0,), 'TargetTextHeight': (12.0,), 'BackgroundColor': ['0x00000000'], 'TextColor': ['0x00555555'], 'TextColorIntensified': ['0x00FFFFFF'], 'IconHighlightMode': (1,), 'IconHighlightFactor': (0.5,), 'FirstActiveMenu': 1, 'StatusBarSize': 10.0, 'StatusBarTextHeight': 4.0, 'TextToSpell': 'AH71K', 'TextResult': '', 'TestMode': 0, 'DestinationAddress': '', 'TextWindowEnabled': 0, 'TextWindowLeft': 600, 'TextWindowTop': 5, 'TextWindowWidth': 512, 'TextWindowHeight': 512, 'TextWindowFontName': 'Courier', 'TextWindowFontSize': 10, 'TextWindowFilePath': '', 'KeystrokeStateName': '', 'KeystrokeExpression': '', 'KeystrokeExpressionOnStartRun': '', 'ConnectorOutputAddress': 'localhost:5001', 'Language': 'Default', 'LocalizedStrings': [['Zeit abgelaufen!', 'Warte ...']], 'ApplicationPort': '50473', 'ApplicationIP': '127.0.0.1', 'ApplicationVersion': [['3.05'], [', '], ['MSVC-15.0.30729.1-AMD64, release, Aug 20 2015 15:13:41, lokinou@DESKTOP-LNFB303'], ['USE_DYNAMIC_IMPORTS:ON USE_SSE2:OFF USE_PRECOMPILED_HEADERS:ON']], 'ApplicationFilterChain': [['ConnectorInput', '2.9999'], ['P3SpellerTask', '3'], ['KeystrokeFilter', '3.1'], ['ConnectorOutput', '3.9999']], 'SpatialFilterType': 0, 'SpatialFilter': [['1', '0', '0', '0'], ['0', '1', '0', '0'], ['0', '0', '1', '0'], ['0', '0', '0', '1']], 'SpatialFilterCAROutput': [], 'SpatialFilterMissingChannels': 1, 'EpochLength': 600, 'EpochsToAverage': 15, 'SingleEpochMode': 0, 'VisualizeP3TemporalFiltering': 1, 'TargetERPChannel': 1, 'Classifier': [['1', '4', '1', '1'], ['1', '6', '2', '1']], 'SignalProcessingPort': '50474', 'SignalProcessingIP': '127.0.0.1', 'SignalProcessingVersion': [['3.05'], [', '], ['MSVC-15.0.30729.1-AMD64, release, Aug 20 2015 15:13:41, lokinou@DESKTOP-LNFB303'], ['USE_DYNAMIC_IMPORTS:ON USE_SSE2:OFF USE_PRECOMPILED_HEADERS:ON']], 'SignalProcessingFilterChain': [['SpatialFilter', '2.A'], ['P3TemporalFilter', '2.B'], ['LinearClassifier', '2.C']], 'VisualizeSpatialFilter': 0, 'VisualizeLinearClassifier': 0, 'RandomSeed': 27348, 'RandomizationWarning': 1}\n",
      "Creating RawArray with float64 data, n_channels=10, n_times=57824\n",
      "    Range : 0 ... 57823 =      0.000 ...   225.871 secs\n",
      "Ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: failed to interpret \"auto\" as type int in parameter \"VisualizeSourceBufferSize\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb stimuli=1050\n",
      "nb targets=150\n",
      "Extracted 5 trials\n"
     ]
    }
   ],
   "source": [
    "nb_stimlus_rows = None  # stores the number of rows in the P300 to separate rows and columns\n",
    "os.path.exists(data_dir)\n",
    "fnames = []\n",
    "for file in os.listdir(data_dir):\n",
    "    if file.endswith(\".gdf\"):\n",
    "        data_type = 'openvibe'\n",
    "        fnames.append(os.path.join(data_dir, file))\n",
    "        print(os.path.join(data_dir, file))\n",
    "    elif file.endswith(\".dat\"):\n",
    "        data_type = 'bci2000'\n",
    "        print(os.path.join(data_dir, file))\n",
    "        fnames.append(os.path.join(data_dir, file))\n",
    "print(fnames)\n",
    "if data_type == 'openvibe':\n",
    "    # load and preprocess data ####################################################\n",
    "    raws = [mne.io.read_raw_gdf(f, preload=True) for f in fnames]\n",
    "elif data_type == 'bci2000':\n",
    "    raws, (nb_stimlus_rows, nb_stimulus_cols, nb_seq) = load_bci2k(fnames, verbose=True)\n",
    "raw = mne.concatenate_raws(raws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29780a12-ad87-46f7-b00a-8992734229e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_stimulus_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebeb81d-d08c-4a59-a013-fe1ecb6dda0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053f9a3b-9862-47d9-9eb4-f3e618e76379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58513759-e223-4bd4-8f6d-9da09a7b1e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "<table class=\"table table-hover\">\n",
       "    <tr>\n",
       "        <th>Measurement date</th>\n",
       "<td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Experimenter</th>\n",
       "<td>Unknown</td>\n",
       "    </tr>\n",
       "        <th>Participant</th>\n",
       "<td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Digitized points</th>\n",
       "        <td>Not available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Good channels</th>\n",
       "        <td>0 magnetometer, 0 gradiometer,\n",
       "            and 10 EEG channels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Bad channels</th>\n",
       "        <td></td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>EOG channels</th>\n",
       "        <td>Not available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>ECG channels</th>\n",
       "        <td>Not available</td>\n",
       "    <tr>\n",
       "        <th>Sampling frequency</th>\n",
       "        <td>256.00 Hz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Highpass</th>\n",
       "        <td>0.00 Hz</td>\n",
       "    </tr>\n",
       "     <tr>\n",
       "        <th>Lowpass</th>\n",
       "        <td>128.00 Hz</td>\n",
       "    </tr>\n",
       "\n",
       "    <tr>\n",
       "        <th>Filenames</th>\n",
       "        <td>bp2calib.dat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Duration</th>\n",
       "        <td>00:03:45 (HH:MM:SS)</td>\n",
       "    </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<RawArray | bp2calib.dat, 10 x 57824 (225.9 s), ~4.4 MB, data loaded>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "471aa253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rescaled signal to Volt (mean variance=2.749515417312947e-10)\n"
     ]
    }
   ],
   "source": [
    "# If the variance of the data is >1, it means the data is expressed in microvolts\n",
    "# Since MNE uses Volt as a default value, we rescale microvolts to volt\n",
    "if np.var(raw._data)>1:\n",
    "    raw._data = raw._data * 1.e-6\n",
    "    print('Rescaled signal to Volt (mean variance={})'.format(np.var(raw._data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4518983d",
   "metadata": {},
   "source": [
    "Create a name for figures output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eaf9aa6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figures will have the name: bp2calib\n"
     ]
    }
   ],
   "source": [
    "output_name = Path(raw._filenames[0]).stem\n",
    "if len(raw._filenames)>1:\n",
    "    output_name = output_name + '_{}_files'.format(len(raw._filenames))\n",
    "print('Figures will have the name: {}'.format(output_name))\n",
    "\n",
    "fig_folder = os.path.join(fig_folder,output_name)\n",
    "if not os.path.exists(fig_folder):\n",
    "    os.mkdir(fig_folder)\n",
    "    print('Creating output directory'.format(fig_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4461997c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if display_preprocessing_plots:\n",
    "    raw.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca97b8f2",
   "metadata": {},
   "source": [
    "## Resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b534c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "if apply_resample:\n",
    "    raw.resample(resample_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60242a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using user defined channel names: ['Fz', 'FC1', 'FC2', 'C1', 'Cz', 'C2', 'P3', 'Pz', 'P4', 'Oz']\n",
      "Electrode mapping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Fz': 'eeg',\n",
       " 'FC1': 'eeg',\n",
       " 'FC2': 'eeg',\n",
       " 'C1': 'eeg',\n",
       " 'Cz': 'eeg',\n",
       " 'C2': 'eeg',\n",
       " 'P3': 'eeg',\n",
       " 'Pz': 'eeg',\n",
       " 'P4': 'eeg',\n",
       " 'Oz': 'eeg'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "montage = mne.channels.make_standard_montage('standard_1005')\n",
    "\n",
    "if cname is None:\n",
    "    cname = raw.info['ch_names']\n",
    "    print('Using channel names directly from the data files: {}'.format(cname))\n",
    "else:\n",
    "    print('Using user defined channel names: {}'.format(cname))\n",
    "        \n",
    "\n",
    "nb_chan = len(raw.info['ch_names'])\n",
    "nb_def_ch = len(cname)\n",
    "\n",
    "# regular expressions to look for certain channel types\n",
    "eog_pattern = re.compile('eog|EOG')  # EOG electrodes\n",
    "emg_pattern = re.compile('emg|EMG')  # EMG electrodes\n",
    "mastoid_pattern = re.compile('^[aA][0-9]+')  # A1 and A2 electrodes (mastoids)\n",
    "\n",
    "\n",
    "types = []\n",
    "cname_map = dict(zip(raw.info['ch_names'], cname))\n",
    "for nc in cname:\n",
    "    if eog_pattern.search(nc) is not None:\n",
    "        t = 'eog'\n",
    "    elif emg_pattern.search(nc) is not None:\n",
    "        t = 'emg'\n",
    "    elif mastoid_pattern.search(nc) is not None:\n",
    "        t = 'misc'\n",
    "    elif nc in montage.ch_names:  # check the 10-05 montage for all electrode names\n",
    "        t = 'eeg'\n",
    "    else:\n",
    "        t = 'misc'  # if not found, discard the channel as misc\n",
    "    \n",
    "    types.append(t)\n",
    "        \n",
    "type_map = dict(zip(cname, types))\n",
    "\n",
    "# rename and pick eeg\n",
    "raw.rename_channels(cname_map, allow_duplicates=False)\n",
    "raw.set_channel_types(type_map)\n",
    "raw.pick_types(eeg=True, misc=False)\n",
    "print('Electrode mapping')\n",
    "type_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8475591f",
   "metadata": {},
   "source": [
    "rename the electrodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bdc3cc",
   "metadata": {},
   "source": [
    "Set the montage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46773eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = raw.set_montage(montage, match_case=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1199c75a",
   "metadata": {},
   "source": [
    "Check whether there are bad channels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ef84aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective window size : 1.000 (s)\n",
      "50Hz variance: []\n",
      "Variance: []\n"
     ]
    }
   ],
   "source": [
    "# Using 50Hz power variance\n",
    "psd, freqs = mne.time_frequency.psd_welch(raw,verbose=True)\n",
    "power_50hz = psd[:,np.where(freqs ==60)]\n",
    "print('50Hz variance: {}'.format(mne.preprocessing.bads._find_outliers(power_50hz.squeeze(), threshold=3, max_iter=5, tail=0)))\n",
    "\n",
    "\n",
    "# using variance\n",
    "ch_var  = [np.var(raw._data[i,:]) for i in list(range(raw._data.shape[0]))]\n",
    "print('Variance: {}'.format(mne.preprocessing.bads._find_outliers(ch_var, threshold=3, max_iter=5, tail=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16e9f7e",
   "metadata": {},
   "source": [
    "rereferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a932dc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if apply_infinite_reference:\n",
    "    raw.del_proj()  # remove our average reference projector first\n",
    "    sphere = mne.make_sphere_model('auto', 'auto', raw.info)\n",
    "    src = mne.setup_volume_source_space(sphere=sphere, exclude=30., pos=15.)\n",
    "    forward = mne.make_forward_solution(raw.info, trans=None, src=src, bem=sphere)\n",
    "    raw_rest = raw.copy().set_eeg_reference('REST', forward=forward)\n",
    "    \n",
    "    if display_preprocessing_plots:\n",
    "        for title, _raw in zip(['Original', 'REST (âˆž)'], [raw, raw_rest]):\n",
    "            fig = _raw.plot(n_channels=len(raw), scalings=dict(eeg=5e-5))\n",
    "            # make room for title\n",
    "            fig.subplots_adjust(top=0.9)\n",
    "            fig.suptitle('{} reference'.format(title), size='xx-large', weight='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3ce84f",
   "metadata": {},
   "source": [
    "## Bandpass the signal\n",
    "Removes noise and drift from the EEG signal by applying a infinite impulse response (two-pass) filter between .5 and 40Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3dfb0b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 40 Hz\n",
      "\n",
      "IIR filter parameters\n",
      "---------------------\n",
      "Butterworth bandpass zero-phase (two-pass forward and reverse) non-causal filter:\n",
      "- Filter order 16 (effective, after forward-backward)\n",
      "- Cutoffs at 0.50, 40.00 Hz: -6.02, -6.02 dB\n",
      "\n",
      "Setting up band-stop filter from 49 - 51 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandstop filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 49.38\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 49.12 Hz)\n",
      "- Upper passband edge: 50.62 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 50.88 Hz)\n",
      "- Filter length: 1691 samples (6.605 sec)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw.filter(.5, 40, fir_window='hann', method='iir')\n",
    "raw.notch_filter(50)  # removes 50Hz noise\n",
    "if display_preprocessing_plots:\n",
    "    raw.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4224b322",
   "metadata": {},
   "source": [
    "## Excluding of channels full of artifacts (muscular or disconnecting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1dd7005e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "Not setting metadata\n",
      "2257 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Loading data for 2257 events and 51 original time points ...\n",
      "0 bad epochs dropped\n",
      "(2257, 10)\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "#Apply a variance based channel rejection if artifacts are present >30% of the time\n",
    "def detec_rej_channel(raw, duration=.2, overlap_duration=.1, threshold_eeg=artifact_threshold, reject_ratio=ratio_tolerated_artifacts):\n",
    "\n",
    "    epochs_rej = mne.make_fixed_length_epochs(raw,duration=duration, overlap=overlap_duration, preload=True)\n",
    "    epochs_rej._data.shape\n",
    "    diff = np.max(epochs_rej._data, axis=2) - np.min(epochs_rej._data, axis=2)\n",
    "\n",
    "    print(diff.shape)\n",
    "\n",
    "    rej = (diff>=threshold_eeg).astype(np.float64)\n",
    "    rel = sns.heatmap(rej)\n",
    "    rel.set(title='Detected artifacts per electrode and runs (white)')\n",
    "\n",
    "    # calculate ratio of rejected trials\n",
    "    ratios = np.sum(rej,axis=0) / rej.shape[0]\n",
    "    \n",
    "    ret = np.argwhere(ratios >= reject_ratio).tolist()\n",
    "    if len(ret)>0 and len(ret[0]):\n",
    "        print('Found {} channels with at least {}% {}s epochs > {} amplitude)'.format(len(ret), \n",
    "                                                                                              reject_ratio*100, duration,\n",
    "                                                                                      threshold_eeg))\n",
    "        return ret[0]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "if reject_channels_full_of_artifacts:\n",
    "    rej_ch = detec_rej_channel(raw)\n",
    "    if rej_ch is not None:\n",
    "        new_bads = [raw.info['ch_names'][ch] for ch in rej_ch]\n",
    "        raw.info['bads'].extend(new_bads)\n",
    "        raw.pick_types(eeg=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4563dccf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Artifact Subspace Reconstruction fitting and reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67c5a650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on samples of size (10, 57824)\n",
      "Model trained\n"
     ]
    }
   ],
   "source": [
    "if apply_ASR:\n",
    "    #!pip install meegkit pymanopt\n",
    "    from meegkit.asr import ASR\n",
    "    fs = int(raw.info[\"sfreq\"])  # sampling frequency\n",
    "    method='riemann'  # if error, use 'euclid'\n",
    "    method='euclid' # pymanopt library still buggy\n",
    "    window_s=.5  # .5 sec window of analysis\n",
    "    data_interval_s  = None # (begin, end) in sec of the training sample\n",
    "    estimator='lwf'  #leave blank if using euclidian mode \n",
    "\n",
    "    # define the ASR model using riemannian method\n",
    "    #asr_model = ASR(sfreq=fs, method=method, win_len=window_s, estimator=estimator)\n",
    "\n",
    "    # if failing (after trying twice. SVD error occurs for no reason sometimes)\n",
    "    asr_model = ASR(sfreq=fs, method=method, win_len=window_s)\n",
    "\n",
    "    # The best would be to choose another recording during the same session to train the model without overfitting\n",
    "    data = raw._data  # the numpy array with data is stored in the _data variable\n",
    "\n",
    "    # Select a time interval for training data\n",
    "    train_idx = None\n",
    "    if data_interval_s is not None:\n",
    "        train_idx = np.arange(data_interval_s[0] * fs, data_interval_s[1] * fs, dtype=int)\n",
    "    # otherwise select the whole training set\n",
    "    else:\n",
    "        train_idx = np.arange(0, data.shape[1])\n",
    "\n",
    "    train_data = data[:, train_idx]\n",
    "    print('Training on samples of size {}'.format(train_data.shape))\n",
    "\n",
    "    # fir the ASR model with data intervals\n",
    "    _, sample_mask = asr_model.fit(train_data)\n",
    "    print('Model trained')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a62943a",
   "metadata": {},
   "source": [
    "### Clean the current dataset\n",
    "Please check whether using this artifact filtering method increases signal to noise ratio rather than reducing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9282725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if apply_ASR:\n",
    "    clean =  asr_model.transform(raw._data)\n",
    "\n",
    "    display_window_s = 60  # \n",
    "\n",
    "    if display_preprocessing_plots:  #\n",
    "        data_p = raw._data[0:fs*display_window_s]  # reshape to (n_chans, n_times)\n",
    "        clean_p = clean[0:fs*display_window_s]\n",
    "\n",
    "        ###############################################################################\n",
    "        # Plot the results\n",
    "        # -----------------------------------------------------------------------------\n",
    "        #\n",
    "        # Data was trained on a 40s window from 5s to 45s onwards (gray filled area).\n",
    "        # The algorithm then removes portions of this data with high amplitude\n",
    "        # artifacts before running the calibration (hatched area = good).\n",
    "        nb_ch_disp = len(raw.info['ch_names'])\n",
    "        times = np.arange(data_p.shape[-1]) / fs\n",
    "        f, ax = plt.subplots(nb_ch_disp, sharex=True, figsize=(32, 16))\n",
    "        for i in range(nb_ch_disp):\n",
    "            # ax[i].fill_between(train_idx / fs, 0, 1, color='grey', alpha=.3,\n",
    "            #                   transform=ax[i].get_xaxis_transform(),\n",
    "            #                   label='calibration window')\n",
    "            # ax[i].fill_between(train_idx / fs, 0, 1, where=sample_mask.flat,\n",
    "            #                   transform=ax[i].get_xaxis_transform(),\n",
    "            #                   facecolor='none', hatch='...', edgecolor='k',\n",
    "            #                   label='selected window')\n",
    "            ax[i].plot(times, data_p[i], lw=.5, label='before ASR')\n",
    "            ax[i].plot(times, clean_p[i], label='after ASR', lw=.5)\n",
    "            # ax[i].plot(times, raw[i]-clean[i], label='Diff', lw=.5)\n",
    "            # ax[i].set_ylim([-50, 50])\n",
    "            ax[i].set_ylabel(f'ch{i}')\n",
    "            ax[i].set_yticks([])\n",
    "        ax[i].set_xlabel('Time (s)')\n",
    "        ax[0].legend(fontsize='small', bbox_to_anchor=(1.04, 1), borderaxespad=0)\n",
    "        plt.subplots_adjust(hspace=0, right=0.75)\n",
    "        plt.suptitle('Before/after ASR')\n",
    "        plt.show()\n",
    "    raw.data_ = clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a02ab2",
   "metadata": {},
   "source": [
    "### Convert text annotations (i.e. unprocessed events) into events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e61c1cd",
   "metadata": {},
   "source": [
    "**Small but major hack to realign events due to conversion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51ba042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_type=='openvibe':\n",
    "    print(\"Erroneous annotations: {}\".format(raw.annotations.description))\n",
    "    print('Note here that the first annotation is 0 or 1, this is an error and thus we shift the annotations to retrieve the correct timings')\n",
    "    raw.annotations.description = np.roll(raw.annotations.description, -1)\n",
    "    print(\"Corrected annotations: {}\".format(raw.annotations.description))\n",
    "\n",
    "    # in case you want to debug the issue, I left here a way to visualize them\n",
    "    # retrieving the list of annotations\n",
    "    import pprint\n",
    "    print(raw.annotations.to_data_frame())\n",
    "    df = raw.annotations.to_data_frame()\n",
    "    print('Displaying all annotations')\n",
    "    annot_codes = [np.int64(n) for n in np.unique(df['description'])]\n",
    "    df['description'] = df['description'].astype(int)\n",
    "\n",
    "    if False:\n",
    "        # to see and debug the fill list of annotations\n",
    "        import pandas as pd\n",
    "        pd.set_option('display.max_rows', None)\n",
    "        #a = df[df['description'] != 33286]\n",
    "        #print(a)\n",
    "        print(df)\n",
    "        pd.set_option('display.max_rows', 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd225eb",
   "metadata": {},
   "source": [
    "### Make a list of the annotations to check whether all stimuli can be found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f06080",
   "metadata": {
    "tags": []
   },
   "source": [
    "These annotations seem to relate to hex codes. OpenViBE definitions can be found on [OpenViBE's website](http://openvibe.inria.fr/stimulation-codes/). Let's parse the copypasted list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cc165b",
   "metadata": {},
   "source": [
    "Make a dataframe of the stimuli in common between both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3d00367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "if data_type=='openvibe':\n",
    "    tr_sim= ''\n",
    "    pat_extract= re.compile('^([^ ]+)[ ]+0x[0-9A-Fa-f]+[ \\/]+([0-9]+)')\n",
    "    #OVTK_GDF_125_Watt                                     0x585       //  1413\n",
    "    k_stim = []\n",
    "    k_stim_int = []\n",
    "    v_stim = []\n",
    "\n",
    "    # read and convert annotations\n",
    "    with open(r'.\\ov_stims.txt', 'r') as fd:\n",
    "        for line in fd.readlines():\n",
    "            m = pat_extract.match(line)\n",
    "            v, k = m.groups()\n",
    "            k_stim.append(k)\n",
    "            k_stim_int.append(int(k))\n",
    "            v_stim.append(v)\n",
    "\n",
    "    # format dict and list\n",
    "    stim_map = dict(zip(k_stim_int, v_stim))\n",
    "    stim_map_inv = dict(zip(v_stim, k_stim))\n",
    "\n",
    "    stim_tup = list(zip(k_stim_int, v_stim))\n",
    "\n",
    "    df = pd.DataFrame.from_dict(stim_tup)\n",
    "    df.columns = ['coden', 'desc']\n",
    "    df[[c in annot_codes for c in df.coden]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bfa34d",
   "metadata": {},
   "source": [
    "From this table, we could locate and save the codes for **Target and Non-Target** and give them the following values: target=1 and non-target=0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb026c75-d5e3-4cab-9123-549923b876cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3f8fbd3-9c5e-453b-bb5c-10c6b4b51b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row labels: ['101' '102' '103' '104' '105' '106']\n",
      "Col labels: ['107' '108' '109' '110' '111' '112' '113' '114']\n"
     ]
    }
   ],
   "source": [
    "# Filter out string annotations\n",
    "string_annotations = np.where(~np.apply_along_axis(np.char.isnumeric,0,raw.annotations.description))[0]\n",
    "if string_annotations.size > 0:\n",
    "    print('removing all string annotations: {}'.format(raw.annotations.description[string_annotations]))\n",
    "    raw.annotations.delete(string_annotations)\n",
    "# Extractz row and column labels from stimuli numbers\n",
    "stimli_labels = np.sort(np.unique(raw.annotations.description[np.where(raw.annotations.description.astype(np.uint8) >= stimulus_padding)]))\n",
    "\n",
    "row_labels = stimli_labels[:nb_stimlus_rows]\n",
    "col_labels = stimli_labels[nb_stimlus_rows:nb_stimlus_rows+nb_stimulus_cols]\n",
    "\n",
    "print(\"Row labels: {}\".format(row_labels))\n",
    "print(\"Col labels: {}\".format(col_labels))\n",
    "\n",
    "# map the stimuli for MNE to read\n",
    "map_stimuli = dict(zip(stimli_labels, stimli_labels.astype(np.uint8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "686637bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 0,\n",
       " '1': 1,\n",
       " '10': 10,\n",
       " '101': 101,\n",
       " '102': 102,\n",
       " '103': 103,\n",
       " '104': 104,\n",
       " '105': 105,\n",
       " '106': 106,\n",
       " '107': 107,\n",
       " '108': 108,\n",
       " '109': 109,\n",
       " '110': 110,\n",
       " '111': 111,\n",
       " '112': 112,\n",
       " '113': 113,\n",
       " '114': 114}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target = 1, nontarget = 0\n",
    "target_map = None\n",
    "if data_type == 'openvibe':\n",
    "    target_map = {'33286':0, '33285':1}\n",
    "elif data_type == \"bci2000\":\n",
    "    target_map = {'0':0, '1':1, '10':10}\n",
    "    target_map.update(map_stimuli)\n",
    "target_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38d6e4e",
   "metadata": {},
   "source": [
    "Then we can convert annotations into events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24830d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['0', '1', '10', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114']\n",
      "Found 2105 events\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'0': 0,\n",
       " '1': 1,\n",
       " '10': 10,\n",
       " '101': 101,\n",
       " '102': 102,\n",
       " '103': 103,\n",
       " '104': 104,\n",
       " '105': 105,\n",
       " '106': 106,\n",
       " '107': 107,\n",
       " '108': 108,\n",
       " '109': 109,\n",
       " '110': 110,\n",
       " '111': 111,\n",
       " '112': 112,\n",
       " '113': 113,\n",
       " '114': 114}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_events, event_id = mne.events_from_annotations(raw, event_id=target_map)\n",
    "print(\"Found {} events\".format(len(all_events[:])))\n",
    "event_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d024a2da",
   "metadata": {},
   "source": [
    "### Pick the channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9983d076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick all channels\n",
    "picks = mne.pick_channels(raw.info[\"ch_names\"], include=[])\n",
    "picks\n",
    "raw.plot_sensors(show_names=True)\n",
    "fig = raw.plot_sensors('3d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbc1d00",
   "metadata": {},
   "source": [
    "## Epoching from events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daf5109-f749-4e39-8a70-e5a340723373",
   "metadata": {},
   "source": [
    "### Prepare metadata to annotate events\n",
    "The goal here is to annotate every event with the Trial, stimulus and isRow information for offline accuracy calculations.\\\n",
    "It takes the shape of a dataframe with the number of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1af8aded-e4dd-4013-ad9b-e062061f4cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trial_nb</th>\n",
       "      <th>stim</th>\n",
       "      <th>is_row</th>\n",
       "      <th>is_col</th>\n",
       "      <th>is_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>106</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>110</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>104</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>107</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>5</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>5</td>\n",
       "      <td>113</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>5</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>5</td>\n",
       "      <td>105</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>5</td>\n",
       "      <td>107</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1050 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Trial_nb stim is_row is_col is_target\n",
       "0           1  106      1      0         0\n",
       "1           1  110      0      1         0\n",
       "2           1  104      1      0         0\n",
       "3           1  114      0      1         0\n",
       "4           1  107      0      1         1\n",
       "...       ...  ...    ...    ...       ...\n",
       "1045        5  109      0      1         1\n",
       "1046        5  113      0      1         0\n",
       "1047        5  108      0      1         0\n",
       "1048        5  105      1      0         0\n",
       "1049        5  107      0      1         0\n",
       "\n",
       "[1050 rows x 5 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the empty dataframe\n",
    "nb_events = np.count_nonzero(np.where(all_events[:,2] <= 1))\n",
    "df_meta = pd.DataFrame(data=None, index=list(range(0,nb_events)), columns=['Trial_nb', 'stim', 'is_row', 'is_col', 'is_target'])\n",
    "ev = all_events.copy()\n",
    "# populate with trial information\n",
    "g_trial = np.split(all_events[:,2], np.where(all_events[:,2]==10)[0])[1:]\n",
    "\n",
    "\n",
    "# iterate over trials to extract their metadata\n",
    "cur_trial_n = 0\n",
    "cursor=0\n",
    "for g in g_trial:\n",
    "    cur_trial_n += 1\n",
    "    #print('Trial {}'.format(cur_trial_n))\n",
    "    idx_stim_labs = np.where(g >= stimulus_padding)  \n",
    "    stim_labels = g[idx_stim_labs]  # extract stimulus labels\n",
    "    # isolate target and non_target stimuli\n",
    "    targets_nontargets_phase = np.delete(g.copy(),idx_stim_labs)\n",
    "    targets_nontargets = np.delete(targets_nontargets_phase,np.where(targets_nontargets_phase == 10))\n",
    "    # extract the stimulus row/column information\n",
    "    is_row = stim_labels <= nb_stimlus_rows+stimulus_padding\n",
    "    \n",
    "    # deduct the cursor over stimulus event\n",
    "    end_cursor = cursor + len(targets_nontargets)\n",
    "    \n",
    "    # build up the metadata dataframe\n",
    "    df_meta.loc[list(range(cursor, end_cursor)),'Trial_nb'] = cur_trial_n\n",
    "    df_meta.loc[list(range(cursor, end_cursor)),'stim'] = stim_labels\n",
    "    df_meta.loc[list(range(cursor, end_cursor)),'is_row'] = is_row.astype(np.uint8)\n",
    "    df_meta.loc[list(range(cursor, end_cursor)),'is_col'] = np.invert(is_row).astype(np.uint8)\n",
    "    df_meta.loc[list(range(cursor, end_cursor)),'is_target'] = targets_nontargets.astype(np.uint8)\n",
    "    \n",
    "    cursor = end_cursor\n",
    "df_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7e2d70-ec2c-4edb-9c8f-42957d035938",
   "metadata": {},
   "source": [
    "### Make epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e51fbe57-4369-43a5-94a3-fa111639cce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding metadata with 5 columns\n",
      "Replacing existing metadata with 5 columns\n",
      "1050 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Loading data for 1050 events and 206 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "# since we use metadata we can pick only target and non-target events\n",
    "events = mne.pick_events(all_events, [0,1])\n",
    "\n",
    "event_ids = dict(NonTarget=0, Target=1) \n",
    "#isi = isi\n",
    "#flash = flash\n",
    "pre_epoch = pre_epoch\n",
    "epoch_length = epoch_length\n",
    "\n",
    "\n",
    "# epoching function\n",
    "epochs = mne.Epochs(raw, events, baseline=baseline, event_id=event_ids, tmin=pre_epoch, tmax=epoch_length, event_repeated='drop', picks = ['eeg', 'csd'],\n",
    "                    preload=True, metadata=df_meta)\n",
    "\n",
    "# if there is any delay,\n",
    "#epochs.shift_time(-isi, relative=True)\n",
    "if display_preprocessing_plots:\n",
    "    fig = epochs.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2898ef39-b918-4c62-9810-4697162934b4",
   "metadata": {},
   "source": [
    "using the metadata, we can now how many events are in our first epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a91f1a",
   "metadata": {},
   "source": [
    "### Making a cross correlation plot between the electrodes to see how channels relate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4a3ff9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.corrcoef(raw._data)\n",
    "fig = plt.figure()\n",
    "hm = sns.heatmap(m,linewidths=0,cmap=\"YlGnBu\").set(title='Cross correlation')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced88097",
   "metadata": {},
   "source": [
    "### Epoch rejection\n",
    "Please filter out channels before epochs. A problematic channel can discard the whole recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e4c142c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "if reject_artifactual_epochs:\n",
    "    reject_criteria = dict(eeg=150e-6)  # 100 ÂµV  #eog=200e-6)\n",
    "    _ = epochs.drop_bad(reject=reject_criteria)\n",
    "    if display_preprocessing_plots:\n",
    "        epochs.plot_drop_log()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d30c19a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Apply current source density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3c3abf42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if apply_CSD:\n",
    "    epochs_csd = mne.preprocessing.compute_current_source_density(epochs)\n",
    "    epochs = epochs_csd\n",
    "    if display_preprocessing_plots:\n",
    "        epochs_csd.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd46a29b",
   "metadata": {},
   "source": [
    "### Average the epochs of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3a68106d",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_nt = epochs['NonTarget'].average()\n",
    "l_target = epochs['Target'].average()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6729d3",
   "metadata": {},
   "source": [
    "target and non target signal plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7500c203",
   "metadata": {},
   "outputs": [],
   "source": [
    "if display_all_erp_plots:\n",
    "    fig, ax = plt.subplots(2, 1)\n",
    "    fig1 = l_target.plot(spatial_colors=True, axes=ax[0], show=False)\n",
    "    fig2 = l_nt.plot(spatial_colors=True, axes=ax[1], show=False)\n",
    "    # Add title\n",
    "    fig.suptitle(\"Target(top) - Non-Target(bottom)\")\n",
    "    # Fix font spacing\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf57d133",
   "metadata": {},
   "source": [
    "target and non target signal topomaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "29bf7661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if display_all_erp_plots:\n",
    "    spec_kw = dict(width_ratios=[1,1,1,.15], wspace=0.5,\n",
    "                   hspace=0.5,height_ratios=[1,1])\n",
    "                             #hspace=0.5, height_ratios=[1, 2])\n",
    "\n",
    "    fig, ax = plt.subplots(2, 4, gridspec_kw=spec_kw)\n",
    "    l_target.plot_topomap(times=[0, 0.18, 0.4], average=0.05, axes=ax[0,:], show=False)\n",
    "    l_nt.plot_topomap(times=[0, 0.18, 0.4], average=0.05, axes=ax[1,:], show=False)\n",
    "    fig.suptitle(\"Target(top) - Non-Target(bottom)\")\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211a88e9",
   "metadata": {},
   "source": [
    "joint plot (of the two former graphs). Plase not that Y scales differ between plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "05494451",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No projector specified for this dataset. Please consider the method self.add_proj.\n",
      "No projector specified for this dataset. Please consider the method self.add_proj.\n"
     ]
    }
   ],
   "source": [
    "l_target.plot_joint()\n",
    "plt.gcf().canvas.set_window_title('Target joint plot')\n",
    "l_nt.plot_joint()\n",
    "plt.gcf().canvas.set_window_title('Non-Target joint plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac1c6bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "Average plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "67f086da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than 6 channels, truncating title ...\n",
      "combining channels using \"mean\"\n",
      "combining channels using \"mean\"\n"
     ]
    }
   ],
   "source": [
    "if display_all_erp_plots:\n",
    "    evokeds = dict(NonTarget=list(epochs['NonTarget'].iter_evoked()), \n",
    "                   Target=list(epochs['Target'].iter_evoked()))\n",
    "    #picks = [f'eeg{n}' for n in range(10, 15)]\n",
    "    mne.viz.plot_compare_evokeds(evokeds, picks=picks, combine='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065fb9b1",
   "metadata": {},
   "source": [
    "### Target vs NonTarget Erps per channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c77ddccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plotting channel 1\n",
      "plotting channel 2\n",
      "plotting channel 3\n",
      "plotting channel 4\n",
      "plotting channel 5\n",
      "plotting channel 6\n",
      "plotting channel 7\n",
      "plotting channel 8\n",
      "plotting channel 9\n",
      "plotting channel 10\n",
      "plotting averaged channels\n",
      "More than 6 channels, truncating title ...\n",
      "combining channels using \"mean\"\n",
      "combining channels using \"mean\"\n",
      "Please note that this plot is optimized for higher resolution and has the legend overlapping the average\n"
     ]
    }
   ],
   "source": [
    "if not skip_slow_ERP_plot:\n",
    "    nb_chans = epochs['Target']._data.shape[1]\n",
    "    splt_width = int(np.floor(np.sqrt(1.0*nb_chans+1)))  # adding an extra plot with all channels combined at the end\n",
    "    splt_height = splt_width if splt_width * splt_width >= nb_chans+1 else splt_width+1\n",
    "    if splt_height * splt_width < nb_chans+1:\n",
    "        splt_height += 1\n",
    "    fig, ax = plt.subplots(splt_height,splt_width)\n",
    "\n",
    "    evokeds = dict(NonTarget=list(epochs['NonTarget'].iter_evoked()), \n",
    "                   Target=list(epochs['Target'].iter_evoked()))\n",
    "    #picks = [f'eeg{n}' for n in range(10, 15)]\n",
    "\n",
    "    shape_epochs = epochs['Target']._data.shape\n",
    "    for ch_idx in range(nb_chans):\n",
    "        print('plotting channel {}'.format(ch_idx+1))\n",
    "        mne.viz.plot_compare_evokeds(evokeds,picks=[epochs.info['ch_names'][ch_idx]],\n",
    "                                     legend=False,\n",
    "                                     axes=ax[ch_idx//splt_width, ch_idx%splt_width], show=False)\n",
    "        #plt.show(block=False)\n",
    "        plt.subplots_adjust(hspace=0.5, wspace=.5)\n",
    "        #plt.pause(.1)\n",
    "    print('plotting averaged channels')\n",
    "    axs = mne.viz.plot_compare_evokeds(evokeds, picks=picks, combine='mean',\n",
    "                                 legend=True,\n",
    "                                 axes=ax[-1,-1], show=False)\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.5, wspace=.5)\n",
    "    plt.show()\n",
    "    print(\"Please note that this plot is optimized for higher resolution and has the legend overlapping the average\")\n",
    "\n",
    "    if export_figures:\n",
    "        out_name = os.path.join(fig_folder, output_name + '_ERPs')\n",
    "        axs[0].savefig(out_name, dpi=300, facecolor='w', edgecolor='w', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9fdec8",
   "metadata": {},
   "source": [
    "### Display single epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dd310e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "Not setting metadata\n",
      "150 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "0 bad epochs dropped\n",
      "combining channels using \"mean\"\n",
      "Not setting metadata\n",
      "Not setting metadata\n",
      "900 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "0 bad epochs dropped\n",
      "combining channels using \"mean\"\n"
     ]
    }
   ],
   "source": [
    "if display_all_erp_plots:\n",
    "    epochs['Target'].plot_image(combine='mean')\n",
    "    plt.gcf().canvas.set_window_title('Target')\n",
    "    epochs['NonTarget'].plot_image(combine='mean')\n",
    "    plt.gcf().canvas.set_window_title('Non-Target')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296c0e3b",
   "metadata": {},
   "source": [
    "### Same plot but channel wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dce1db4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_slow_ERP_plot:\n",
    "    dict_electrodes = dict(eeg='EEG') if not apply_CSD else dict(csd='CSD')\n",
    "    if display_all_erp_plots:\n",
    "        for ch_type, title in dict_electrodes.items():\n",
    "            layout = mne.channels.find_layout(epochs.info, ch_type=ch_type)\n",
    "            epochs['Target'].plot_topo_image(layout=layout, fig_facecolor='w',\n",
    "                                                    font_color='k', title=title+' Target Trial x time amplitude')\n",
    "            epochs['NonTarget'].plot_topo_image(layout=layout, fig_facecolor='w',\n",
    "                                                    font_color='k', title=title+' Non-Target Trial x time amplitude')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f4d9f1",
   "metadata": {},
   "source": [
    "# Classical LDA training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5057efa",
   "metadata": {},
   "source": [
    "resample the signal, we don't need that much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6a299028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resampling to 32Hz\n"
     ]
    }
   ],
   "source": [
    "new_fs = resample_LDA #\n",
    "epochs_resampled = epochs.copy().resample(new_fs)\n",
    "print('resampling to {}Hz'.format(new_fs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee2f656",
   "metadata": {},
   "source": [
    "modify the data matrix to be properly assessed via LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1f99f12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "X = epochs_resampled._data[:,1,:]  # input data at CZ (TODO:flatten all electrodes)\n",
    "X = epochs_resampled._data[:,:,:]  # input data at CZ (TODO:flatten all electrodes)\n",
    "y = epochs_resampled.events[:,2]  # ground truth\n",
    "\n",
    "# remove the information \n",
    "\n",
    "def reshape_mne_raw_for_lda(X, verbose=False):\n",
    "    # reshapes raw data for LDA, flattening the last dimension\n",
    "    #mne.stats.permutation_t_test()\n",
    "    if verbose:\n",
    "        print('Data shape from MNE {}'.format(X.shape))\n",
    "    X_out = np.moveaxis(X,1,-1)\n",
    "    if verbose:\n",
    "        print('new data shape with sampling prioritized over channels {}'.format(X_out.shape))\n",
    "    X_out = X_out.reshape([X_out.shape[0],X_out.shape[1]*X_out.shape[2]],order='C')\n",
    "    if verbose:\n",
    "        print('Shape for K-fold LDA {}'.format(X_out.shape))\n",
    "    return X_out\n",
    "\n",
    "X = reshape_mne_raw_for_lda(X)\n",
    "\n",
    "# Think about splitting training sample and test samples\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4d2a1d",
   "metadata": {},
   "source": [
    "### Compute k-fold LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e4a4f1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold score: 0.976, AUC=0.917\n",
      "fold score: 0.976, AUC=0.972\n",
      "fold score: 0.957, AUC=0.892\n",
      "fold score: 0.967, AUC=0.897\n",
      "fold score: 0.952, AUC=0.861\n",
      "Average score 5-Fold = 0.97, AUC=0.91\n",
      "Score training-validation 0.95, AUC=0.84\n",
      "Score is only valid if classes are balanced, please check AUC instead\n"
     ]
    }
   ],
   "source": [
    "clf = LinearDiscriminantAnalysis(solver='lsqr',shrinkage='auto')\n",
    "kf = KFold(n_splits=nb_k_splits)\n",
    "kf.get_n_splits(X)\n",
    "\n",
    "list_score = []\n",
    "list_auc = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train_kf, X_test_kf = X[train_index], X[test_index]\n",
    "    y_train_kf, y_test_kf = y[train_index], y[test_index]\n",
    "    clf.fit(X_train_kf, y_train_kf)\n",
    "    kscore = clf.score(X_test_kf,y_test_kf)\n",
    "    y_pred_kf = clf.predict(X_test_kf)\n",
    "    k_auc = roc_auc_score(y_test_kf, y_pred_kf)\n",
    "    print('fold score: {}, AUC={}'.format(np.round(kscore, decimals=3), np.round(k_auc, decimals=3)))\n",
    "    list_score.append(kscore)\n",
    "    list_auc.append(k_auc)\n",
    "    \n",
    "print('Average score {}-Fold = {}, AUC={}'.format(kf.get_n_splits(X), np.round(np.mean(list_score), decimals=2), np.round(np.mean(list_auc), decimals=2)))\n",
    "\n",
    "# using the training/validate samples, \n",
    "clf.fit(X_train, y_train)\n",
    "score  = clf.score(X_test,y_test)\n",
    "y_pred = clf.predict(X_test)\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print('Score training-validation {}, AUC={}'.format(np.round(score , decimals=2), np.round(auc, decimals=2)))\n",
    "\n",
    "print('Score is only valid if classes are balanced, please check AUC instead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "24ad1f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data contains 86.0% of Non-targets\n"
     ]
    }
   ],
   "source": [
    "nb_targets = epochs_resampled['Target'].events.shape[0]\n",
    "nb_non_targets = epochs_resampled['NonTarget'].events.shape[0]\n",
    "\n",
    "print('Data contains {}% of Non-targets'.format(np.round(100*(nb_non_targets / (epochs_resampled.events.shape[0])))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fad580",
   "metadata": {},
   "source": [
    "### Display train-test LDA classification in a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "acb4b42e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred 0(Non-Target)</th>\n",
       "      <th>Pred 1(Target)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True 0(Non-Target)</th>\n",
       "      <td>TN = 269 (TNR = 98.53%)</td>\n",
       "      <td>FP = 4 (FPR = 1.47%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True 1(Target)</th>\n",
       "      <td>FN = 13 (FNR = 30.95%)</td>\n",
       "      <td>TP = 29 (TPR = 69.05%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Pred 0(Non-Target)          Pred 1(Target)\n",
       "True 0(Non-Target)  TN = 269 (TNR = 98.53%)    FP = 4 (FPR = 1.47%)\n",
       "True 1(Target)       FN = 13 (FNR = 30.95%)  TP = 29 (TPR = 69.05%)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def conf_matrix(y,pred):\n",
    "    cmat = metrics.confusion_matrix(y, pred)\n",
    "    cmat_norm = metrics.confusion_matrix(y, pred, \n",
    "            normalize='true')\n",
    "    ((tn, fp), (fn, tp)) = cmat\n",
    "    ((tnr,fpr),(fnr,tpr)) = cmat_norm\n",
    "    \n",
    "    plt.figure()\n",
    "    labels = ['Non-Target', 'Target']\n",
    "    sns.heatmap(cmat, xticklabels=labels, yticklabels=labels, annot=True, fmt=\"d\")\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.xlabel('Ground Truth')\n",
    "    plt.ylabel('Predicted')\n",
    "    \n",
    "    # alternative using sklearn plots\n",
    "    #plt.figure()\n",
    "    #from sklearn.metrics import ConfusionMatrixDisplay\n",
    "    #cm_display = ConfusionMatrixDisplay(cmat).plot()\n",
    "    \n",
    "    return pd.DataFrame([[f'TN = {tn} (TNR = {tnr:1.2%})', \n",
    "                                f'FP = {fp} (FPR = {fpr:1.2%})'], \n",
    "                         [f'FN = {fn} (FNR = {fnr:1.2%})', \n",
    "                                f'TP = {tp} (TPR = {tpr:1.2%})']],\n",
    "            index=['True 0(Non-Target)', 'True 1(Target)'], \n",
    "            columns=['Pred 0(Non-Target)', \n",
    "                            'Pred 1(Target)'])\n",
    "conf_matrix(y_test,y_pred)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ba3aed",
   "metadata": {},
   "source": [
    "## Process the ROC curve and precision recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a69a012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "y_score = clf.decision_function(X_test)\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score, pos_label=clf.classes_[1])\n",
    "roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr)#.plot()\n",
    "\n",
    "\n",
    "# Precision Recall Display\n",
    "prec, recall, _ = precision_recall_curve(y_test, y_score,\n",
    "                                         pos_label=clf.classes_[1])\n",
    "pr_display = PrecisionRecallDisplay(precision=prec, recall=recall)#.plot()\n",
    "\n",
    "# Display them side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n",
    "roc_display.plot(ax=ax1)\n",
    "ax1.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "        label='Chance', alpha=.8)\n",
    "ax1.legend(loc=\"lower right\")\n",
    "pr_display.plot(ax=ax2)\n",
    "\n",
    "\n",
    "if export_figures:\n",
    "    out_name = os.path.join(fig_folder, output_name + '_ROC')\n",
    "    fig.savefig(out_name, dpi=300, facecolor='w', edgecolor='w', bbox_inches='tight')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91533fb",
   "metadata": {},
   "source": [
    "## Signed R-Square plot "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64a2cc9",
   "metadata": {},
   "source": [
    "### Use the function from Wyrm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f9c95b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/bbci/wyrm/blob/master/wyrm/processing.py\n",
    "# Bastian Venthur for wyrm\n",
    "# Code initially from Benjamin Blankertz for bbci (Matlab)\n",
    "\n",
    "def calculate_signed_r_square_mne(epochs, classes=[0,1], classaxis=0, **kwargs):\n",
    "    \"\"\"Calculate the signed r**2 values.\n",
    "    This method calculates the signed r**2 values over the epochs of the\n",
    "    ``dat``.\n",
    "    Parameters\n",
    "    ----------\n",
    "    epochs : MNE epoched data\n",
    "    classes: list, optional \n",
    "        (either int index or str for the class name of the epoch))\n",
    "    classaxis : int, optional\n",
    "        the dimension containing epochs\n",
    "    Returns\n",
    "    -------\n",
    "    signed_r_square : ndarray\n",
    "        the signed r**2 values, signed_r_square has one axis less than\n",
    "        the ``dat`` parameter, the ``classaxis`` has been removed\n",
    "    Examples\n",
    "    --------\n",
    "    >>> dat.data.shape\n",
    "    (400, 100, 64)\n",
    "    >>> r = calculate_signed_r_square(dat)\n",
    "    >>> r.shape\n",
    "    (100, 64)\n",
    "    \"\"\"\n",
    "    # TODO: explain the algorithm in the docstring and add a reference\n",
    "    # to a paper.\n",
    "    # select class 0 and 1\n",
    "    # TODO: make class 0, 1 variables\n",
    "    fv1 = epochs[classes[0]]._data\n",
    "    fv2 = epochs[classes[1]]._data\n",
    "    # number of epochs per class\n",
    "    l1 = epochs[classes[0]]._data.shape[classaxis]\n",
    "    l2 = epochs[classes[1]]._data.shape[classaxis]\n",
    "    # calculate r-value (Benjamin approved!)\n",
    "    a = (fv1.mean(axis=classaxis) - fv2.mean(axis=classaxis)) * np.sqrt(l1 * l2)\n",
    "    b = epochs._data.std(axis=classaxis) * (l1 + l2)\n",
    "    r = a / b\n",
    "    # return signed r**2\n",
    "    return np.sign(r) * np.square(r)\n",
    "\n",
    "rsq = calculate_signed_r_square_mne(epochs, classes=['Target','NonTarget'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370a0937",
   "metadata": {},
   "source": [
    "### make a pandas database to properly display electrodes and samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6788f3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a pandas database to properly display electrodes and samples\n",
    "fs = epochs.info['sfreq']\n",
    "x = np.float64(list(range(rsq.shape[1])))*(1000/fs)\n",
    "x = x.round(decimals=0).astype(np.int64) + np.int64(pre_epoch*1000)\n",
    "df_rsq = pd.DataFrame(rsq, columns=x, index=epochs.info['ch_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72663ac7",
   "metadata": {},
   "source": [
    "### Plot rsq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fb8ac6",
   "metadata": {},
   "source": [
    "note that using a larger sampling rate will smooth this figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b4beac2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-53-6951f6bb0cd2>:1: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n"
     ]
    }
   ],
   "source": [
    "plt.figure()\n",
    "hm = sns.heatmap(df_rsq,linewidths=0,cmap=\"coolwarm\").set(title='Signed r-square maps Target vs Non-Target', xlabel='Time (ms)')\n",
    "\n",
    "if export_figures:\n",
    "    out_name = os.path.join(fig_folder, output_name + '_heatmap' )\n",
    "    plt.savefig(out_name, dpi=300, facecolor='w', edgecolor='w', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8339498",
   "metadata": {},
   "source": [
    "### Quickly Display a channel with max rsq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "91e3ed55",
   "metadata": {},
   "outputs": [],
   "source": [
    "picks = None  # <- specify the channel here or it will be selected automatically\n",
    "if picks is None:\n",
    "    ch_max, _ = np.where(rsq == np.max(rsq))\n",
    "    picks = epochs.info['ch_names'][int(ch_max)]\n",
    "\n",
    "#picks = [f'eeg{n}' for n in range(10, 15)]\n",
    "evokeds = dict(NonTarget=list(epochs['NonTarget'].iter_evoked()), \n",
    "               Target=list(epochs['Target'].iter_evoked()))\n",
    "axs = mne.viz.plot_compare_evokeds(evokeds, picks=picks)  # use combine='mean' if several electrode chosen in picks\n",
    "\n",
    "if export_figures:\n",
    "    out_name = os.path.join(fig_folder, output_name + '_best_channel')\n",
    "    axs[0].savefig(out_name, dpi=300, facecolor='w', edgecolor='w', bbox_inches='tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bdce3c",
   "metadata": {},
   "source": [
    "# Extract offline analysis, using shrinkage LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fc2f5c-2adf-4c0d-9cc6-10910cfef750",
   "metadata": {},
   "source": [
    "### Extract correct target pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "00c135b0-7b63-4d05-99b2-ee8b217f5cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_target(epochs, trial_nb):\n",
    "    #epoch_trial = epochs[epochs.metadata['Trial_nb'] == trial_nb]\n",
    "    assert (epochs.metadata['Trial_nb'] == trial_nb).size > 0, 'Trial number not found in metadata'\n",
    "    idx_targets = np.where(np.logical_and(epochs.metadata['Trial_nb'] == trial_nb, \n",
    "                                          epochs.metadata['is_target']))[0]\n",
    "    # extract target or target pair\n",
    "    target_pair = np.sort(np.unique(epochs.metadata.loc[idx_targets]['stim'])).astype(np.uint8)\n",
    "    # remove the stimulus pardding to give it a meaning\n",
    "    return target_pair - stimulus_padding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a8cb36-da5c-45ff-8d29-b42c0bcc1e89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33b18e0f-56b7-4cef-90cb-9d720dd84442",
   "metadata": {},
   "source": [
    "Extract ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b7377074-ffdd-443a-831b-5af962425aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cumulative(clf, X, step):\n",
    "    \"\"\"\n",
    "    Repeats a function on the first axis on the 2d data\"\"\"\n",
    "    array_out = [clf.predict(X[0:cursor_seq,:]) for cursor_seq in np.arange(0,X.shape[0], step) + step]\n",
    "    return array_out\n",
    "\n",
    "def predict_proba_cumulative(clf, X, step):\n",
    "    \"\"\"\n",
    "    Repeats a function on the first axis on the 2d data\"\"\"\n",
    "    array_out = [clf.predict_proba(X[0:cursor_seq,:]) for cursor_seq in np.arange(0,X.shape[0], step) + step]\n",
    "    return array_out\n",
    "\n",
    "def score_cumulative(clf, X, y,  step):\n",
    "    \"\"\"\n",
    "    Repeats a function on the first axis on the 2d data\"\"\"\n",
    "    array_out = [clf.score(X[0:cursor_seq,:], y[0:cursor_seq]) for cursor_seq in np.arange(0,X.shape[0], step) + step]\n",
    "    return array_out\n",
    "\n",
    "def auc_cumulative(y, y_preds_cumulative,  step):\n",
    "    array_out = []\n",
    "    for i in range(0, y.shape[0]//step):\n",
    "        y_pred = y_preds_cumulative[i]\n",
    "        y_gt = y[0:step*(i+1)]\n",
    "        roc = roc_auc_score(y_gt, y_pred)\n",
    "        array_out.append(roc)\n",
    "        \n",
    "    #one liner not working TT\n",
    "    #array_out = [roc_auc_score(y[0:cursor_seq], y_preds_cumulative[(cursor_seq//step)-1]) for cursor_seq in np.arange(0,y.shape[0], step) + step]\n",
    "    return array_out\n",
    "\n",
    "# Transform into a function\n",
    "\n",
    "def stim_from_predict_cumulative(predicted, stim_labels):\n",
    "    \"\"\"\n",
    "    predicted: takes the output of predict_proba and returns the predicted target\n",
    "    stim_labels: stimulus labels from the epochs \n",
    "    \"\"\"\n",
    "    # retrieve the target probability index\n",
    "    max_proba_class = np.argmax(predicted,axis=1)\n",
    "    if np.max(max_proba_class) == 0:\n",
    "        print('(note:LDA detected no target when classifying n={} epochs, using best target candidate instead)'.format(predicted.shape[0]))\n",
    "        max_proba_class[[np.argmax(proba_cum_rows[0][:,1])]] = 1\n",
    "    # extract probabilities\n",
    "    max_proba = np.array([predicted[idx,max_proba_class[idx]] for idx in list(range(predicted.shape[0]))])\n",
    "    # target_indices\n",
    "    target_epochs_idx = np.where(max_proba_class)\n",
    "    # extract target stimuli from the list of stimuli and put them in a table with their respective probability\n",
    "    predicted_target_stims = stim_labels[target_epochs_idx].astype(np.uint8)\n",
    "    pred_targets_table = np.vstack((predicted_target_stims, max_proba[target_epochs_idx]))\n",
    "    # sum up target probability in this table stimulis-wise (thus dealing with situations with draws)\n",
    "    potential_targets = np.unique(pred_targets_table[0,:])\n",
    "    sum_proba_targets = [np.sum(pred_targets_table[1,np.where(pred_targets_table[0,:]==target_candidate)]) for target_candidate in potential_targets]\n",
    "    pred_targets_table_reduced = np.vstack((potential_targets, sum_proba_targets))\n",
    "\n",
    "    # return the stimulation with the highest average probability\n",
    "    predicted_stim = pred_targets_table_reduced[0,np.argmax(pred_targets_table_reduced[1,:])].astype(np.uint8)\n",
    "    \n",
    "    return predicted_stim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1c1d598a-d1e7-4e9b-bc79-4c83daa4a016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resampling to 32Hz\n"
     ]
    }
   ],
   "source": [
    "new_fs = resample_LDA #\n",
    "epochs_resampled = epochs.copy().resample(new_fs)\n",
    "print('resampling to {}Hz'.format(new_fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "519cb288-04f6-4c75-a173-636048978213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_trials:5, nb_folds:5\n"
     ]
    }
   ],
   "source": [
    "# Check whether we can proceed to k split\n",
    "list_trials = np.unique(epochs_resampled.metadata[\"Trial_nb\"])\n",
    "\n",
    "print(\"nb_trials:{}, nb_folds:{}\".format(list_trials.size, nb_k_splits))\n",
    "assert list_trials.size // nb_k_splits == list_trials.size / nb_k_splits, 'number of splits must be a multiple of the number of trials'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "086d4435-bda4-40fd-a0a4-087d4ba70c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0 partial score: 0.976, AUC=0.917\n",
      "fold 1 partial score: 0.976, AUC=0.972\n",
      "fold 2 partial score: 0.957, AUC=0.892\n",
      "fold 3 partial score: 0.967, AUC=0.897\n",
      "fold 4 partial score: 0.952, AUC=0.861\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fold</th>\n",
       "      <th>fold_trial</th>\n",
       "      <th>n_seq</th>\n",
       "      <th>score</th>\n",
       "      <th>AUC</th>\n",
       "      <th>row_true</th>\n",
       "      <th>row_pred</th>\n",
       "      <th>col_true</th>\n",
       "      <th>col_pred</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101</td>\n",
       "      <td>101</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.75</td>\n",
       "      <td>101</td>\n",
       "      <td>101</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>101</td>\n",
       "      <td>101</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.875</td>\n",
       "      <td>101</td>\n",
       "      <td>101</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.9</td>\n",
       "      <td>101</td>\n",
       "      <td>101</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0.961039</td>\n",
       "      <td>0.901515</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>109</td>\n",
       "      <td>109</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>109</td>\n",
       "      <td>109</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>109</td>\n",
       "      <td>109</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0.959184</td>\n",
       "      <td>0.886905</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>109</td>\n",
       "      <td>109</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>109</td>\n",
       "      <td>109</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    fold  fold_trial  n_seq     score       AUC  row_true  row_pred  col_true  \\\n",
       "0      1           1      1       1.0       1.0       101       101       107   \n",
       "1      1           1      2  0.928571      0.75       101       101       107   \n",
       "2      1           1      3  0.952381  0.833333       101       101       107   \n",
       "3      1           1      4  0.964286     0.875       101       101       107   \n",
       "4      1           1      5  0.971429       0.9       101       101       107   \n",
       "..   ...         ...    ...       ...       ...       ...       ...       ...   \n",
       "70     5           1     11  0.961039  0.901515       102       102       109   \n",
       "71     5           1     12  0.958333  0.888889       102       102       109   \n",
       "72     5           1     13  0.961538  0.897436       102       102       109   \n",
       "73     5           1     14  0.959184  0.886905       102       102       109   \n",
       "74     5           1     15  0.952381  0.861111       102       102       109   \n",
       "\n",
       "    col_pred  correct  \n",
       "0        107        1  \n",
       "1        107        1  \n",
       "2        107        1  \n",
       "3        107        1  \n",
       "4        107        1  \n",
       "..       ...      ...  \n",
       "70       109        1  \n",
       "71       109        1  \n",
       "72       109        1  \n",
       "73       109        1  \n",
       "74       109        1  \n",
       "\n",
       "[75 rows x 10 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LinearDiscriminantAnalysis(solver='lsqr',shrinkage='auto')\n",
    "\n",
    "kf = KFold(n_splits=nb_k_splits)\n",
    "\n",
    "# make a table to store the scores and accuracies\n",
    "score_table = pd.DataFrame({'fold' : pd.Series([], dtype='int'),\n",
    "                            'fold_trial' : pd.Series([], dtype='int'),\n",
    "                            'n_seq' : pd.Series([], dtype='int'),\n",
    "                            'score' : pd.Series([], dtype='float'),\n",
    "                            'AUC' : pd.Series([], dtype='float'),\n",
    "                            'row_true' : pd.Series([], dtype='str'),\n",
    "                            'row_pred' : pd.Series([], dtype='str'),\n",
    "                            'col_true' : pd.Series([], dtype='str'),\n",
    "                            'col_pred' : pd.Series([], dtype='str'),\n",
    "                            'correct' : pd.Series([], dtype='int')})\n",
    "\n",
    "fold_counter = 0\n",
    "for train_index, test_index in kf.split(list_trials):\n",
    "    a = test_index\n",
    "    \n",
    "    \n",
    "    # define the training sample\n",
    "    X_train = epochs_resampled[epochs_resampled.metadata['Trial_nb'].isin(list_trials[train_index])]._data\n",
    "    y_train = epochs_resampled[epochs_resampled.metadata['Trial_nb'].isin(list_trials[train_index])].metadata['is_target'].astype(np.uint8)\n",
    "    y_train_stim = epochs_resampled[epochs_resampled.metadata['Trial_nb'].isin(list_trials[train_index])].metadata['stim']\n",
    "\n",
    "    # reshape data to enter LDA\n",
    "    X_train = reshape_mne_raw_for_lda(X_train)\n",
    "    \n",
    "    # train the LDA classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # split the tests samples for each individual trial enable P300 speller target prediction and accuracy\n",
    "    nb_test_trials_in_split = len(list_trials)\n",
    "    for i_trial in list(range(len(test_index))):\n",
    "        trial_nb = test_index[i_trial]+1\n",
    "    \n",
    "        # both row and columns for generic classification\n",
    "        epoch_test = epochs_resampled[epochs_resampled.metadata['Trial_nb'] == trial_nb]\n",
    "        X_test = epoch_test._data\n",
    "        y_test = epoch_test[epoch_test.metadata['Trial_nb'] == trial_nb].metadata['is_target'].astype(np.uint8)\n",
    "\n",
    "        # separate row and columns target detection\n",
    "        epoch_rows = epoch_test[np.where(epoch_test.metadata['is_row'])[0]]\n",
    "        epoch_cols = epoch_test[np.where(epoch_test.metadata['is_col'])[0]]\n",
    "        \n",
    "        skip_rows = False\n",
    "        if np.where(epoch_rows.metadata['is_target']==1)[0].size == 0:\n",
    "            print('Skipping epochs with no target rows')\n",
    "            skip_rows = True\n",
    "        else:\n",
    "            X_test_rows = epoch_rows[epoch_rows.metadata['Trial_nb'] == trial_nb]._data\n",
    "            y_test_rows = epoch_rows[epoch_rows.metadata['Trial_nb'] == trial_nb].metadata['is_target'].astype(np.uint8)\n",
    "            y_test_rows_stim = epoch_rows[epoch_rows.metadata['Trial_nb'] == trial_nb].metadata['stim']\n",
    "            X_test_rows = reshape_mne_raw_for_lda(X_test_rows)  # reshape for LDA\n",
    "            \n",
    "        skip_cols = False\n",
    "        if np.where(epoch_cols.metadata['is_target']==1)[0].size == 0:\n",
    "            skip_cols = True\n",
    "            print('Skipping epochs with no target columns')\n",
    "        else:\n",
    "            X_test_cols = epoch_cols[epoch_cols.metadata['Trial_nb'] == trial_nb]._data\n",
    "            y_test_cols = epoch_cols[epoch_cols.metadata['Trial_nb'] == trial_nb].metadata['is_target'].astype(np.uint8)\n",
    "            y_test_cols_stim = epoch_cols[epoch_cols.metadata['Trial_nb'] == trial_nb].metadata['stim']\n",
    "            X_test_cols = reshape_mne_raw_for_lda(X_test_cols)  # reshape\n",
    "\n",
    "\n",
    "        # reshape data to enter LDA\n",
    "        X_test = reshape_mne_raw_for_lda(X_test)\n",
    "        \n",
    "       \n",
    "\n",
    "        # predict the targets for rows, (1 to N sequences cumulative X information provided)\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        # update this when dealing with rows and columns to X_test_rows and X_test_cols\n",
    "        step = X_test.shape[0]//nb_seq\n",
    "\n",
    "        # score\n",
    "        score_cum = score_cumulative(clf, X_test, y_test, step) \n",
    "        # probas calculated for prediction (aggregation will lead to cumulative prediction)\n",
    "        #proba_cum = proba_cumulative(clf, X_test, y_test, step)\n",
    "        y_test_pred_cum = predict_cumulative(clf, X_test, step)\n",
    "        # area under the curve\n",
    "        auc_cum = auc_cumulative(y_test, y_test_pred_cum, step)\n",
    "\n",
    "        if not skip_rows:\n",
    "            # prediction stim_rows\n",
    "            step_rows = X_test_rows.shape[0]//nb_seq\n",
    "            proba_cum_rows = predict_proba_cumulative(clf, X_test_rows, step_rows)\n",
    "            pred_stim_rows_cum = np.array([stim_from_predict_cumulative(proba_cum_rows[i], epoch_rows.metadata['stim'].to_numpy())\n",
    "                                  for i in list(range(len(proba_cum_rows)))])\n",
    "            true_target_rows = np.unique(epoch_rows.metadata.iloc[np.where(epoch_rows.metadata['is_target']==1)]['stim'])[0]  # does not handle k fold with several trials\n",
    "            rows_successul = pred_stim_rows_cum == true_target_rows\n",
    "            \n",
    "        if not skip_cols:\n",
    "            # columns\n",
    "            step_cols = X_test_cols.shape[0]//nb_seq\n",
    "            proba_cum_cols = predict_proba_cumulative(clf, X_test_cols, step_cols)\n",
    "            pred_stim_cols_cum = np.array([stim_from_predict_cumulative(proba_cum_cols[i], epoch_cols.metadata['stim'].to_numpy())\n",
    "                                  for i in list(range(len(proba_cum_cols)))])\n",
    "            true_target_cols = np.unique(epoch_cols.metadata.iloc[np.where(epoch_cols.metadata['is_target']==1)[0]]['stim'])[0]  # does not handle k fold with several trials\n",
    "            cols_successul = pred_stim_cols_cum == true_target_cols\n",
    "            \n",
    "            \n",
    "        if not skip_rows and not skip_cols:\n",
    "            successful_pred_cum = np.logical_and(rows_successul, cols_successul).astype(np.uint8)\n",
    "        elif skip_rows and skip_cols:\n",
    "            raise\n",
    "        elif skip_rows:\n",
    "            successful_pred_cum = cols_successul\n",
    "        elif skip_cols:\n",
    "            successful_pred_cum = rows_successul\n",
    "\n",
    "\n",
    "        # Associate predicted targets to stimuli\n",
    "        for i in range(len(score_cum)):\n",
    "            line = dict(zip(score_table.columns, [fold_counter+1, i_trial+1, i+1, # fold, fold_trial, nb of sequences\n",
    "                                                  score_cum[i], \n",
    "                                                  auc_cum[i],true_target_rows, pred_stim_rows_cum[i],\n",
    "                                                  true_target_cols, pred_stim_cols_cum[i], successful_pred_cum[i]]))\n",
    "\n",
    "            score_table = score_table.append(line, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "        print('fold {} partial score: {}, AUC={}'.format(fold_counter, np.round(score_cum[-1], decimals=3), np.round(auc_cum[-1], decimals=3)))\n",
    "\n",
    "        #print('----row score: {}, AUC={}'.format(np.round(kscore_rows, decimals=3), np.round(auc_rows, decimals=3)))\n",
    "        #print('----col score: {}, AUC={}'.format(np.round(kscore_cols, decimals=3), np.round(auc_cols, decimals=3)))\n",
    "    fold_counter += 1\n",
    "    \n",
    "# clear the types or the score table\n",
    "score_table.convert_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0cbb8e35-35a0-4b8e-a6f0-96c3c49ed9eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ERP targets=150, non-targets=900\n"
     ]
    }
   ],
   "source": [
    "df_seq = score_table.groupby(['n_seq']).mean()\n",
    "df_seq = df_seq.rename(columns={\"correct\":\"Accuracy\", \"score\":\"epoch_score\", \"AUC\":\"epoch_AUC\"})\n",
    "df_seq[['Accuracy','epoch_score', 'epoch_AUC']]\n",
    "\n",
    "df_seq.plot(y='Accuracy')\n",
    "plt.ylim(0, 1.05)\n",
    "plt.suptitle('Cross-fold offline accuracy (n={})'.format(nb_k_splits))\n",
    "plt.xlabel('Number of sequences')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['P300 target prediction (row x col)'])\n",
    "# export the figure\n",
    "out_name = os.path.join(fig_folder, output_name + '_accuracy')\n",
    "plt.savefig(out_name, dpi=300, facecolor='w', edgecolor='w', bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Number of ERP targets={}, non-targets={}\".format(epochs['Target']._data.shape[0], epochs['NonTarget']._data.shape[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a799d34-1094-4dbd-a396-306b18fe7f78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdfcd28-830f-4440-850c-3ab6eee10c60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
